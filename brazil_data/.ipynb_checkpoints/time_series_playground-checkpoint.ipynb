{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e03fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d82d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path,data_file,file_type=\"csv\",unchange_indices=[],indirect_indices=[],\n",
    "                direct_indices=[],id_ind=0,target_ind=-1,val_prop=0.10,test_prop=0.10,\n",
    "                opt_params={},save_file=\"\"):\n",
    "\n",
    "    \"\"\"\n",
    "        data_path: Path to the data file. The output data will be written to this \n",
    "\t\t   location.\n",
    "\n",
    "        data_file: File containing the data to be loaded.\n",
    "\n",
    "        file_type: The type of file, either 'csv' or 'pkl'.\n",
    "\n",
    "                   (1 ) 'csv' assumes the following:\n",
    "\n",
    "                         a. Has a header and is the first line in the file.\n",
    "                         b. The first column identifies the instance.\n",
    "                         c. The last column is the target variable.\n",
    "                         d. ALL VARIABLES ARE NUMERIC (including identifiers\n",
    "                            and target).\n",
    "\n",
    "                   (2) 'pkl' file type is assumed to have been generated\n",
    "                        according to this code.\n",
    "        \n",
    "        unchange_indices: The indices onf the unchangeable features.\n",
    "\n",
    "        indirect_indices: The indices of the indirectly changeable features.\n",
    " \n",
    "        direct_indices: The indices of the directly changeable features\n",
    "\n",
    "        seed: Seed to randomly partition data.\n",
    "\n",
    "        val_prop: Proportion of data to be used for the validation set.\n",
    "\n",
    "        test_prop: Proportion of data to be used for the test set.\n",
    "\n",
    "        imbal_classes: Boolean. Whether or not there is class imbalance. If\n",
    "                       set to True, then we will stratify the positive class\n",
    "                       (assumed to be the imbalanced class). To ensure that\n",
    "                       positive samples are present in the train, validation,\n",
    "                       and test sets.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if file_type == \"pkl\":\n",
    "        with open(data_path+data_file,'rb') as rF:\n",
    "            load_data = pkl.load(rF)\n",
    "            return load_data\n",
    "    \n",
    "    elif file_type == \"csv\":\n",
    "        sep=\",\"\n",
    "    else:\n",
    "        raise Exception(\"Unsupoorted file type {}. Support file types are 'csv' and 'pkl'.\".format(file_type))\n",
    "\n",
    "    dset_df = pd.read_csv(data_path+data_file,sep=sep)\n",
    "\n",
    "    header = dset_df.columns\n",
    "\n",
    "\n",
    "    id_col_name = header[id_ind]\n",
    "    target_col_name = header[target_ind]\n",
    "    indirect_col_names = header[indirect_indices]\n",
    "    direct_col_names = header[direct_indices]\n",
    "    unchange_col_names = header[unchange_indices]\n",
    "\n",
    "    dset_ids = dset_df[id_col_name].values\n",
    "    dset_targets = dset_df[target_col_name].values\n",
    "    X_data = dset_df.drop([id_col_name, target_col_name],axis=1)\n",
    "    \n",
    "\n",
    "    unchange_indices = [X_data.columns.get_loc(c) for c in unchange_col_names]\n",
    "    indirect_indices = [X_data.columns.get_loc(c) for c in indirect_col_names]\n",
    "    direct_indices = [X_data.columns.get_loc(c) for c in direct_col_names]\n",
    "\n",
    "    X_data = X_data.values    \n",
    "\n",
    "\n",
    "    #Define train, val, test indices according to test_prop, val_prop\n",
    "    n = dset_ids.shape[0]\n",
    "    train_indices = [i for i in range(int(n*(1 - test_prop - val_prop)))]\n",
    "    val_indices = [i + int(n*(1 - test_prop - val_prop)) for i in range(int(n*val_prop))]\n",
    "    test_indices = [i + int(n*(1 - test_prop)) for i in range(int(n*test_prop))]\n",
    "\n",
    "    #Partition data into train,val,test according to the above defined indices\n",
    "    \n",
    "    #Train\n",
    "    train_X = X_data[train_indices]    \n",
    "    train_target = dset_targets[train_indices]\n",
    "    train_ids = dset_ids[train_indices]\n",
    "\n",
    "    #Obtain normalization values\n",
    "    min_X = np.amin(train_X,axis=0)\n",
    "    max_X = np.amax(train_X,axis=0) \n",
    "\n",
    "    #Normalize training data\n",
    "    norm_train_X =np.divide(train_X - min_X,max_X - min_X)\n",
    "    \n",
    "   \n",
    "    train_dict = {\"X\":norm_train_X, \"target\":train_target, \"ids\":train_ids}\n",
    "\n",
    "    #Val\n",
    "    val_X= X_data[val_indices]\n",
    "    val_target = dset_targets[val_indices]\n",
    "    val_ids = dset_ids[val_indices]\n",
    "\n",
    "    #Normailze validation data\n",
    "    norm_val_X = np.divide(val_X - min_X,max_X - min_X)\n",
    "\n",
    "    val_dict = {\"X\":norm_val_X, \"target\":val_target, \"ids\":val_ids}\n",
    "\n",
    "    #Test\n",
    "    test_X = X_data[test_indices]\n",
    "    test_target = dset_targets[test_indices]\n",
    "    test_ids = dset_ids[test_indices]\n",
    "\n",
    "    #Normalize test data\n",
    "    norm_test_X = np.divide(test_X - min_X,max_X - min_X)\n",
    "\n",
    "\n",
    "    test_dict = {\"X\":norm_test_X, \"target\":test_target, \"ids\":test_ids}\n",
    "\n",
    "\n",
    "    return_dict = {'train':train_dict,\n",
    "                   'val':val_dict,\n",
    "                   'test':test_dict,\n",
    "                   'train_indices':train_indices,\n",
    "                   'val_indices':val_indices,\n",
    "                   'test_indices':test_indices,\n",
    "                   'min_X':min_X,\n",
    "                   'max_X':max_X,\n",
    "                   'opt_params':opt_params,\n",
    "                   'xU_ind':unchange_indices,\n",
    "                   'xI_ind':indirect_indices,\n",
    "                   'xD_ind':direct_indices\n",
    "                   }\n",
    "\n",
    "    #If a save file is defined, write the defined data out.\n",
    "    if save_file != \"\":\n",
    "        with open(data_path+save_file,'wb') as sF:\n",
    "            pkl.dump(return_dict,sF)\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff43c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indices(data_path,util_file):\n",
    "    \"\"\"\n",
    "        data_path: Path to data files.\n",
    "\n",
    "        util_file: Name of the file containing the index designations, cost\n",
    "                   parameters, and direction of change parameters. Should be\n",
    "                    of the form:\n",
    "\n",
    "                        index, designation, cost increase, cost decrease, direction\n",
    "                      \n",
    "                        e.g.:\n",
    "\n",
    "                        0,id,,,\n",
    "                        1,dir,0,2,-1\n",
    "                        2,dir,3,0,1\n",
    "                        3,dir,4,3,0\n",
    "                        4,unch,,,\n",
    "                        5,ind,,,\n",
    "                         ...\n",
    "                        p,target,,,\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    unch_indices = []\n",
    "    ind_indices = []\n",
    "    dir_indices = []\n",
    "    cost_inc = []\n",
    "    cost_dec = []\n",
    "    direct_chg = []\n",
    "    id_ind = -1\n",
    "    target_ind = -1\n",
    "    with open(data_path+util_file,'rU') as rF:\n",
    "        fReader = csv.reader(rF,delimiter=',')\n",
    "        for i, row in enumerate(fReader):\n",
    "            if row[1] == 'id':\n",
    "                id_ind = int(row[0])\n",
    "            elif row[1] == 'target':\n",
    "                target_ind = int(row[0])\n",
    "            elif row[1] == 'ind':\n",
    "                ind_indices.append(int(row[0]))\n",
    "            elif row[1] == 'unch':\n",
    "                unch_indices.append(int(row[0]))\n",
    "            elif row[1] == 'dir':\n",
    "                dir_indices.append(int(row[0]))\n",
    "                cost_inc.append(int(row[2]))\n",
    "                cost_dec.append(int(row[3]))\n",
    "                direct_chg.append(int(row[4]))\n",
    "            else:\n",
    "                raise Exception(\"Problem loading index file. Unrecognized designation '{}' found on row\\\n",
    "                          {}\".format(row[0],str(i+1)))\n",
    "\n",
    "    return unch_indices,ind_indices,dir_indices,cost_inc,cost_dec,direct_chg,id_ind,target_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdd4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "unch_indices,indir_indices,dir_indices,cost_inc,cost_dec,direct_chg,id_ind,target_ind = load_indices('','brazil_indices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f84037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = {'cost_inc':cost_inc,'cost_dec':cost_dec,'direct_chg':direct_chg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d56a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_data('','brazil_weather.csv','csv',\n",
    "                          unch_indices,indir_indices,dir_indices,id_ind=id_ind,\n",
    "                          target_ind=target_ind,val_prop=0.10,test_prop=0.10,\n",
    "                          opt_params=opt_params,save_file=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9af200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat = data_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a658ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edfc19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = train_dat['X'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a39acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e641901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, data_dict):\n",
    "        \n",
    "        self.train_dat = data_dict['train']['X']\n",
    "        self.val_dat = data_dict['val']['X']\n",
    "        self.test_dat = data_dict['test']['X']\n",
    "        \n",
    "        self.label_column_indices = data_dict['xD_ind']\n",
    "        self.column_indices = data_dict['xU_ind'] + data_dict['xI_ind'] + data_dict['xD_ind']\n",
    "        \n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        \n",
    "        self.total_window_size = input_width + shift\n",
    "        \n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "        \n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.label_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.label_slice]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column indices: {self.label_column_indices}'\n",
    "        ])\n",
    "    \n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.label_slice, :]\n",
    "        labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[i]] for i in self.label_column_indices],\n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "        \n",
    "        return inputs, labels\n",
    "    \n",
    "    def plot(self, model=None, plot_col=17, max_subplots=3):\n",
    "        inputs, labels = self.example\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        max_n = min(max_subplots, inputs.shape[0])\n",
    "\n",
    "        plot_col_index = self.column_indices.index(plot_col)\n",
    "        label_col_index = self.label_column_indices.index(plot_col)\n",
    "\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'Column {plot_col_index} [normed]')\n",
    "            plt.plot(\n",
    "                self.input_indices,\n",
    "                inputs[n, :, plot_col_index],\n",
    "                label='Inputs', marker='.', zorder=-10\n",
    "            )\n",
    "            plt.scatter(\n",
    "                self.label_indices,\n",
    "                labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64\n",
    "            )\n",
    "\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(\n",
    "                    self.label_indices,\n",
    "                    predictions[n, :, label_col_index],\n",
    "                    marker='X', edgecolors='k', label='Predictions',\n",
    "                    c='#ff7f0e', s=64\n",
    "                )\n",
    "\n",
    "            if n==0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e2d2446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 48\n",
       "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
       "Label indices: [47]\n",
       "Label column indices: [17, 18, 19, 20, 21, 22, 23, 24]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = WindowGenerator(input_width=24, label_width=1, shift=24, data_dict=data_dict)\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c96792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 7\n",
       "Input indices: [0 1 2 3 4 5]\n",
       "Label indices: [6]\n",
       "Label column indices: [17, 18, 19, 20, 21, 22, 23, 24]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = WindowGenerator(input_width=6, label_width=1, shift=1, data_dict=data_dict)\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9067de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_window = tf.stack([\n",
    "    train_dat['X'][:w2.total_window_size],\n",
    "    train_dat['X'][100:100+w2.total_window_size],\n",
    "    train_dat['X'][200:200+w2.total_window_size]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae65608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All shapes are: (batch, time, features)\n",
      "Window shape: (3, 7, 25)\n",
      "Inputs shape: (3, 6, 25)\n",
      "Labels shape: (3, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "example_inputs, example_labels = w2.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'Labels shape: {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbe3f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d48e6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.example = example_inputs, example_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5904e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_labels[1, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47010c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
